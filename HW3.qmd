---
title: "Homework 3"
author: "[Robert Chappell]{style='background-color: yellow;'}"
toc: true
title-block-banner: true
title-block-style: default
# format: html
format: pdf
---

[Link to the Github repository](https://github.com/psu-stat380/hw-3)

---

::: {.callout-important style="font-size: 0.8em;"}
## Due: Thu, Mar 2, 2023 @ 11:59pm

Please read the instructions carefully before submitting your assignment.

1. This assignment requires you to only upload a `PDF` file on Canvas
1. Don't collapse any code cells before submitting. 
1. Remember to make sure all your code output is rendered properly before uploading your submission.

⚠️ Please add your name to the author information in the frontmatter before submitting your assignment ⚠️
:::

For this assignment, we will be using the [Wine Quality](https://archive.ics.uci.edu/ml/datasets/wine+quality) dataset from the UCI Machine Learning Repository. The dataset consists of red and white _vinho verde_ wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests

We will be using the following libraries:

```{R}
library(readr)
library(tidyr)
library(dplyr)
library(purrr)
library(car)
library(glmnet)
```

<br><br><br><br>
---

## Question 1
::: {.callout-tip}
## 50 points
Regression with categorical covariate and $t$-Test
:::

###### 1.1 (5 points)

Read the wine quality datasets from the specified URLs and store them in data frames `df1` and `df2`.

```{R}
url1 <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"

url2 <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"


df1 <- read.csv(url1, sep=";")
df2 <- read.csv(url2, sep=";")
```

---

###### 1.2 (5 points)

Perform the following tasks to prepare the data frame `df` for analysis:

1. Combine the two data frames into a single data frame `df`, adding a new column called `type` to indicate whether each row corresponds to white or red wine. 
1. Rename the columns of `df` to replace spaces with underscores
1. Remove the columns `fixed_acidity` and `free_sulfur_dioxide`
1. Convert the `type` column to a factor
1. Remove rows (if any) with missing values.


```{R}
df1 <- df1 %>%
  mutate(type = 'white')
df2 <- df2 %>%
  mutate(type = 'red')


df <- rbind(df1, df2)

colnames(df) <- gsub("\\.", "_", colnames(df))

df <- df %>%
  select(-c(fixed_acidity, free_sulfur_dioxide))

df$type <- as.factor(df$type)

df <- na.omit(df)

dim(df)

```


Your output to `R dim(df)` should be
```
[1] 6497   11
```



---

###### 1.3 (20 points)

Recall from STAT 200, the method to compute the $t$ statistic for the the difference in means (with the equal variance assumption)

1. Using `df` compute the mean of `quality` for red and white wine separately, and then store the difference in means as a variable called `diff_mean`. 

2. Compute the pooled sample variance and store the value as a variable called `sp_squared`. 

3. Using `sp_squared` and `diff_mean`, compute the $t$ Statistic, and store its value in a variable called `t1`.


```{R}
diff_mean <- mean(df$quality[df$type == "red"]) - mean(df$quality[df$type == "white"])


n_red <- sum(df$type == "red")
n_white <- sum(df$type == "white")
var_red <- var(df$quality[df$type == "red"])
var_white <- var(df$quality[df$type == "white"])
sp_squared <- ((n_red - 1) * var_red + (n_white - 1) * var_white) / (n_red + n_white - 2)


t1 <- diff_mean / sqrt(sp_squared * (1/n_red + 1/n_white))
```


---

###### 1.4 (10 points)

Equivalently, R has a function called `t.test()` which enables you to perform a two-sample $t$-Test without having to compute the pooled variance and difference in means. 

Perform a two-sample t-test to compare the quality of white and red wines using the `t.test()` function with the setting `var.equal=TRUE`. Store the t-statistic in `t2`.

```{R}
t_test <- t.test(quality ~ type, data = df, var.equal = TRUE)
t2 <- t2 <- unname(t.test(quality ~ type, data = df, var.equal = TRUE)$statistic)
```

---

###### 1.5 (5 points)

Fit a linear regression model to predict `quality` from `type` using the `lm()` function, and extract the $t$-statistic for the `type` coefficient from the model summary. Store this $t$-statistic in `t3`.

```{R}
fit <- lm(quality ~ type, data = df) # Insert your here
t3 <- summary(fit)$coefficients[2, "t value"] # Insert your here
```


---

###### 1.6  (5 points)

Print a vector containing the values of `t1`, `t2`, and `t3`. What can you conclude from this? Why?

```{R}
c(t1, t2, t3) # Insert your code here
```

From the vector you can conclude that the true t value is 9.68565, the positive or negative does not matter.


<br><br><br><br>
<br><br><br><br>
---

## Question 2
::: {.callout-tip}
## 25 points
Collinearity
:::


---

###### 2.1 (5 points)

Fit a linear regression model with all predictors against the response variable `quality`. Use the `broom::tidy()` function to print a summary of the fitted model. What can we conclude from the model summary?


```{R}
fit <- lm(quality ~ ., data = df)
broom::tidy(fit)
```

From the model summary, we can conclude that the predictor variables volatile_acidity, citric_acid, alcohol, and type have significant coefficients. However, other variables such as residual_sugar and chlorides do not have significant coefficients.

---

###### 2.2 (10 points)

Fit two **simple** linear regression models using `lm()`: one with only `citric_acid` as the predictor, and another with only `total_sulfur_dioxide` as the predictor. In both models, use `quality` as the response variable. How does your model summary compare to the summary from the previous question?


```{R}
model_citric <- lm(quality ~ citric_acid, data = df)
summary(model_citric)
```

```{R}
model_sulfur <- lm(quality ~ total_sulfur_dioxide, data = df)
summary(model_sulfur)
```

When comparing the models to the previous summary, we can see that the $r^2$ values are much lower than the previous model, meaning these variables are not a strong predictor of quality alone.

---

###### 2.3 (5 points)

Visualize the correlation matrix of all numeric columns in `df` using `corrplot()`

```{R}
library(corrplot)
df %>% 
  select_if(is.numeric) %>%
  cor() %>%
  corrplot()
```



---

###### 2.4 (5 points)

Compute the variance inflation factor (VIF) for each predictor in the full model using `vif()` function. What can we conclude from this?


```{R}
vif(fit)
```

From this you can see that variables such as density and type are higher meaning that they are not as useful in predicting quality, and could possibly be removed.

<br><br><br><br>
<br><br><br><br>
---

## Question 3
::: {.callout-tip}
## 40 points

Variable selection
:::


---

###### 3.1 (5 points)

Run a backward stepwise regression using a `full_model` object as the starting model. Store the final formula in an object called `backward_formula` using the built-in `formula()` function in R

```{R}
full_model <- full_model <- lm(quality ~ ., data = df)
backward <- step(full_model, direction = 'backward')
backward_formula <- formula(backward)
```

---

###### 3.2 (5 points)

Run a forward stepwise regression using a `null_model` object as the starting model. Store the final formula in an object called `forward_formula` using the built-in `formula()` function in R

```{R}
null_model <- lm(quality ~ 1, data = df)
forward <- step(null_model, direction = 'forward')
forward_formula <- formula(forward)
```



---

###### 3.3  (10 points)

1. Create a `y` vector that contains the response variable (`quality`) from the `df` dataframe. 

2. Create a design matrix `X` for the `full_model` object using the `make_model_matrix()` function provided in the Appendix. 

3. Then, use the `cv.glmnet()` function to perform LASSO and Ridge regression with `X` and `y`.

```{R}
make_model_matrix <- function(formula){
  X <- model.matrix(formula, df)[, -1]
  cnames <- colnames(X)
  for(i in 1:ncol(X)){
    if(!cnames[i] == "typewhite"){
      X[, i] <- scale(X[, i])
    } else {
      colnames(X)[i] <- "type"
    }
  }
  return(X)
}



y <- df$quality

X <- make_model_matrix(full_model)

lasso_cv <- cv.glmnet(X, y, alpha = 1, nfolds = 10, type.measure = "mse")
ridge_cv <- cv.glmnet(X, y, alpha = 0, nfolds = 10, type.measure = "mse")
```

Create side-by-side plots of the ridge and LASSO regression results. Interpret your main findings. 

```{R}
par(mfrow=c(1, 2))
plot(lasso_cv)
plot(ridge_cv)
```

The plot for LASSO typically shows a sharper decline in MSE than the plot for Ridge regression, indicating that LASSO performs more variable selection than Ridge regression.

---

###### 3.4  (5 points)

Print the coefficient values for LASSO regression at the `lambda.1se` value? What are the variables selected by LASSO? 

Store the variable names with non-zero coefficients in `lasso_vars`, and create a formula object called `lasso_formula` using the `make_formula()` function provided in the Appendix. 

```{R}
make_formula <- function(x){
  as.formula(
    paste("quality ~ ", paste(x, collapse = " + "))
  )
}

coef(lasso_cv, s = "lambda.1se")
lasso_vars <- rownames(coef(lasso_cv))[-1][coef(lasso_cv, s = "lambda.1se")[-1] != 0]
lasso_formula <- make_formula(c("1", lasso_vars))
```

The variables with non zero coefficients are the ones chosen.

---

###### 3.5  (5 points)

Print the coefficient values for ridge regression at the `lambda.1se` value? What are the variables selected here? 

Store the variable names with non-zero coefficients in `ridge_vars`, and create a formula object called `ridge_formula` using the `make_formula()` function provided in the Appendix. 

```{R}
coef(ridge_cv, s = "lambda.1se")
ridge_vars <- rownames(coef(ridge_cv))[-1][coef(ridge_cv, s = "lambda.1se")[-1] != 0]
ridge_formula <- make_formula(c("1", ridge_vars))
```

All variables are selected.

---

###### 3.6  (10 points)

What is the difference between stepwise selection, LASSO and ridge based on you analyses above?

The difference is the amount of variables chose, LASSO removes the most variables. Ridge does not remove any and stepwise only removed 2, not as many as LASSO.





<br><br><br><br>
<br><br><br><br>
---

## Question 4
::: {.callout-tip}
## 70 points

Variable selection
:::

---

###### 4.1  (5 points)

Excluding `quality` from `df` we have $10$ possible predictors as the covariates. How many different models can we create using any subset of these $10$ coavriates as possible predictors? Justify your answer. 

Given 10 possible predictors, we have two options for each predictor - include it in the model or exclude it. So for each predictor, we have two choices, either include it in the model or exclude it. This gives us a total of $2^{10} = 1024$ possible models.

---


###### 4.2  (20 points)

Store the names of the predictor variables (all columns except `quality`) in an object called `x_vars`.

```{R}
x_vars <- colnames(df %>% select(-quality))
```

Use: 

* the `combn()` function (built-in R function) and 
* the `make_formula()` (provided in the Appendix) 

to **generate all possible linear regression formulas** using the variables in `x_vars`. This is most optimally achieved using the `map()` function from the `purrr` package.

```{R}
formulas <- map(
  1:length(x_vars),
  \(x){
    vars <- combn(x_vars, x, simplify = FALSE) # Insert code here
    map(vars, function(y) make_formula(y)) # Insert code here
  }
) %>% unlist()
```

If your code is right the following command should return something along the lines of:

```{R}
sample(formulas, 4) %>% as.character()
# Output:
# [1] "quality ~ volatile_acidity + residual_sugar + density + pH + alcohol"                                                 
# [2] "quality ~ citric_acid"                                                                                                
# [3] "quality ~ volatile_acidity + citric_acid + residual_sugar + total_sulfur_dioxide + density + pH + sulphates + alcohol"
# [4] "quality ~ citric_acid + chlorides + total_sulfur_dioxide + pH + alcohol + type"  
```

---

###### 4.3  (10 points)
Use `map()` and `lm()` to fit a linear regression model to each formula in `formulas`, using `df` as the data source. Use `broom::glance()` to extract the model summary statistics, and bind them together into a single tibble of summaries using the `bind_rows()` function from `dplyr`.

```{R}

models <- map(formulas, ~lm(.x, data = df)) # Insert your code here
summaries <- map(models,  broom::glance) %>% bind_rows() # Insert your code here
```



---


###### 4.4  (5 points)

Extract the `adj.r.squared` values from `summaries` and use them to identify the formula with the _**highest**_ adjusted R-squared value.

```{R}
index <- which.max(summaries$adj.r.squared)
```

Store resulting formula as 
a variable called `rsq_formula`.

```{R}
rsq_formula <- formulas[index]
```

---

###### 4.5  (5 points)

Extract the `AIC` values from `summaries` and use them to identify the formula with the **_lowest_** AIC value.


```{R}
lowest_aic_index <- which.min(summaries$AIC)
```

Store resulting formula as a variable called `aic_formula`.


```{R}
aic_formula <- formulas[lowest_aic_index]
```

---

###### 4.6  (15 points)

Combine all formulas shortlisted into a single vector called `final_formulas`.

```{R}
null_formula <- formula(null_model)
full_formula <- formula(full_model)

final_formulas <- c(
  null_formula,
  full_formula,
  backward_formula,
  forward_formula,
  lasso_formula, 
  ridge_formula,
  rsq_formula,
  aic_formula
)
```

* Are `aic_formula` and `rsq_formula` the same? How do they differ from the formulas shortlisted in question 3?

They are not the same, aic was the minimized aic value, and rsq was the highest adjusted r squared value. Compared to the formulas in question three, the aic and rsq formulas used those and other factors to minimize the rsq.

* Which of these is more reliable? Why? 

They are both very reliable, but aic is more reliable with a small sample size compared to the number of predictors, and max r squared is large sample size in comparison to the predictors. for this dataset the rsq formula is more reliable.

* If we had a dataset with $10,000$ columns, which of these methods would you consider for your analyses? Why?

I would use aic formula, because of the large amount of predictors, we would need to perform variable selection to minimize the aic.

---

###### 4.7  (10 points)


Use `map()` and `glance()` to extract the `sigma, adj.r.squared, AIC, df`, and `p.value` statistics for each model obtained from `final_formulas`. Bind them together into a single data frame `summary_table`. Summarize your main findings.

```{R}
summary_table <- map(
  final_formulas, ~ lm(.x, data = df) %>% broom::glance() %>% select(sigma, adj.r.squared, AIC, df, p.value)
) %>% bind_rows()

summary_table %>% knitr::kable()
```


The first and fourth models, which have the same sigma and adj.r.squared values, correspond to the null model and do not have any predictor variables. The AIC value for both these models is highest, which indicates that they have the worst fit.

The second and third models have the lowest AIC value, indicating that they have the best fit among all the models. These models also have higher adj.r.squared values compared to other models, indicating that they explain a significant proportion of the variation in the response variable.

The fifth and seventh models have df values less than the other models, indicating that they have fewer predictor variables. However, they have relatively higher AIC values compared to the second and third models, indicating that they have a poorer fit.

The sixth model, which is the Ridge regression model, has the highest sigma value among all the models, indicating that it has a larger amount of residual variation.


:::{.hidden unless-format="pdf"}
\pagebreak
:::

<br><br><br><br>
<br><br><br><br>
---


# Appendix


#### Convenience function for creating a formula object

The following function which takes as input a vector of column names `x` and outputs a `formula` object with `quality` as the response variable and the columns of `x` as the covariates. 

```{R}
make_formula <- function(x){
  as.formula(
    paste("quality ~ ", paste(x, collapse = " + "))
  )
}

# For example the following code will
# result in a formula object
# "quality ~ a + b + c"
make_formula(c("a", "b", "c"))
```

#### Convenience function for `glmnet`

The `make_model_matrix` function below takes a `formula` as input and outputs a **rescaled** model matrix `X` in a format amenable for `glmnet()`

```{R}
make_model_matrix <- function(formula){
  X <- model.matrix(formula, df)[, -1]
  cnames <- colnames(X)
  for(i in 1:ncol(X)){
    if(!cnames[i] == "typewhite"){
      X[, i] <- scale(X[, i])
    } else {
      colnames(X)[i] <- "type"
    }
  }
  return(X)
}
```




::: {.callout-note collapse="true"}
## Session Information

Print your `R` session information using the following command

```{R}
sessionInfo()
```
:::